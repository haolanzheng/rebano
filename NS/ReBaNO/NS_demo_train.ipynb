{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Device: cpu\n",
      "Expected Final GPT-PINN Depth: [3, 4, 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import and GPU Support\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "\n",
    "from NS_data import create_residual_data, create_ICBC_data\n",
    "from NS_Plotting import Burgers_plot, loss_plot \n",
    "\n",
    "# Full PINN\n",
    "from NS_PINN import PINN\n",
    "from NS_PINN_train import pinn_train\n",
    "\n",
    "# Burgers GPT-PINN\n",
    "from NS_GPT_activation import P\n",
    "from NS_GPT_precomp import autograd_calculations, Pt_nu_lap_vor\n",
    "from NS_GPT_PINN import GPT\n",
    "from NS_GPT_train import gpt_train\n",
    "\n",
    "\n",
    "torch.set_default_dtype(torch.float)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Current Device: {device}\")\n",
    "\n",
    "\n",
    "n_train = 1000\n",
    "\n",
    "nu = 0.025\n",
    "# Domain and Data\n",
    "Xi, Xf = 0.0, 2*np.pi\n",
    "Yi, Yf = Xi, Xf\n",
    "Ti, Tf = 0.0, 10.0\n",
    "Nc, N_test     = 64, 64\n",
    "Nt = 64\n",
    "BC_pts, IC_pts = 100, 64\n",
    "\n",
    "residual_data = create_residual_data(Xi, Xf, Yi, Yf, Ti, Tf, Nc, Nt, N_test)\n",
    "xt_resid      = residual_data[0].to(device)\n",
    "xt_test       = residual_data[1].to(device) \n",
    "\n",
    "ICBC_data = create_ICBC_data(Xi, Xf, Yi, Yf, Ti, Tf, Nt, BC_pts, IC_pts)\n",
    "IC_xt     = ICBC_data[0].to(device)\n",
    "BC_xt_bottom = ICBC_data[1].to(device)\n",
    "BC_xt_top    = ICBC_data[2].to(device)\n",
    "BC_xt_left   = ICBC_data[3].to(device)\n",
    "BC_xt_right  = ICBC_data[4].to(device)\n",
    "\n",
    "# Training Parameter Set\n",
    "K = 100\n",
    "Nx = 64\n",
    "path = \"../data/\"\n",
    "# rf_coef = np.load(path + fr\"Random_NS_rf_coef_{K}_{Nx}.npy\")\n",
    "curl_f = np.load(path + fr'Random_NS_curl_f_{K}_{Nx}.npy').transpose(1, 0, 2)\n",
    "omega0 = np.load(path + fr\"Random_NS_omega0_{K}_{Nx}.npy\").T\n",
    "\n",
    "\n",
    "\n",
    "# xi_train = torch.from_numpy(rf_coef[:, :, :n_train].astype(np.float32)).to(device)\n",
    "omega0 = torch.from_numpy(omega0.reshape(-1,1).astype(np.float32)).to(device)\n",
    "curl_f_train = torch.from_numpy(curl_f[:, :, :n_train].astype(np.float32)).to(device)\n",
    "\n",
    "train_final_gpt   = True\n",
    "number_of_neurons = 4\n",
    "loss_list         = np.ones(number_of_neurons)\n",
    "print(f\"Expected Final GPT-PINN Depth: {[3,number_of_neurons,1]}\\n\")\n",
    "\n",
    "###############################################################################\n",
    "#################################### Setup ####################################\n",
    "###############################################################################\n",
    "\n",
    "P_list = np.ones(number_of_neurons, dtype=object)\n",
    "\n",
    "lr_adam  = 0.005\n",
    "lr_lbfgs = 0.8\n",
    "epochs_pinn  = 60000\n",
    "# epochs_lbfgs = 1000\n",
    "\n",
    "layers_pinn = np.array([3, 20, 20, 20, 20, 20, 20, 20, 20, 2])\n",
    "tol_adam    = 1e-7\n",
    "\n",
    "lr_gpt          = 0.005\n",
    "epochs_gpt      = 2000\n",
    "epochs_gpt_test = 5000\n",
    "test_cases      = 200\n",
    "\n",
    "# Save Data/Plot Options\n",
    "save_data         = True\n",
    "plot_pinn_loss    = False\n",
    "plot_pinn_sol     = False\n",
    "plot_largest_loss = False\n",
    "\n",
    "#generate mesh to find U0-pred for the whole domain\n",
    "\n",
    "P_resid_vorstr_values = torch.ones((xt_resid.shape[0], 2, number_of_neurons)).to(device)\n",
    "P_IC_values    = torch.ones((   IC_xt.shape[0], number_of_neurons)).to(device)\n",
    "P_BC_bottom    = torch.ones((   BC_xt_bottom.shape[0], 2, number_of_neurons)).to(device)\n",
    "P_BC_top       = torch.ones((   BC_xt_bottom.shape[0], 2, number_of_neurons)).to(device)\n",
    "P_BC_left      = torch.ones((   BC_xt_bottom.shape[0], 2, number_of_neurons)).to(device)\n",
    "P_BC_right     = torch.ones((   BC_xt_bottom.shape[0], 2, number_of_neurons)).to(device)\n",
    "\n",
    "P_t_term  = torch.ones((xt_resid.shape[0], number_of_neurons)).to(device)\n",
    "P_x_term  = torch.ones((xt_resid.shape[0], number_of_neurons)).to(device)\n",
    "P_xx_term = torch.ones((xt_resid.shape[0], number_of_neurons)).to(device)\n",
    "P_y_term  = torch.ones((xt_resid.shape[0], number_of_neurons)).to(device)\n",
    "P_yy_term = torch.ones((xt_resid.shape[0], number_of_neurons)).to(device)\n",
    "P_vel     = torch.ones((xt_resid.shape[0], 2, number_of_neurons)).to(device)\n",
    "P_lap_psi = torch.ones((xt_resid.shape[0], number_of_neurons)).to(device)\n",
    "Pt_nu_lap_omega = torch.ones((xt_resid.shape[0], number_of_neurons)).to(device)\n",
    "\n",
    "x_resid = xt_resid[:, 0:1].to(device)\n",
    "y_resid = xt_resid[:, 1:2].to(device)\n",
    "t_resid = xt_resid[:, 2:3].to(device)\n",
    "\n",
    "IC_x = IC_xt[:, 0:1].to(device)\n",
    "IC_y = IC_xt[:, 1:2].to(device)\n",
    "IC_t = IC_xt[:, 2:3].to(device)\n",
    "BC_x_top = BC_xt_top[:, 0:1].to(device)\n",
    "BC_y_top = BC_xt_top[:, 1:2].to(device)\n",
    "BC_t_top = BC_xt_top[:, 2:3].to(device)\n",
    "BC_x_bottom = BC_xt_bottom[:, 0:1].to(device)\n",
    "BC_y_bottom = BC_xt_bottom[:, 1:2].to(device)\n",
    "BC_t_bottom = BC_xt_bottom[:, 2:3].to(device)\n",
    "BC_x_left = BC_xt_left[:, 0:1].to(device)\n",
    "BC_y_left = BC_xt_left[:, 1:2].to(device)\n",
    "BC_t_left = BC_xt_left[:, 2:3].to(device)\n",
    "BC_x_right = BC_xt_right[:, 0:1].to(device)\n",
    "BC_y_right = BC_xt_right[:, 1:2].to(device)\n",
    "BC_t_right = BC_xt_right[:, 2:3].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************\n",
      "Begin Full PINN Training: case=0 (Obtaining Neuron 1)\n",
      "Epoch: 0 | Loss: 0.1904820054769516\n",
      "Epoch: 10000 | loss: 0.0004257224209140986\n",
      "Epoch: 20000 | loss: 1.5508367141592316e-06\n",
      "Epoch: 30000 | loss: 5.657966539729387e-07\n",
      "Epoch: 40000 | loss: 4.284432634449331e-06\n",
      "Epoch: 50000 | loss: 1.2431302138793399e-06\n",
      "Epoch: 60000 | loss: 9.153779501502868e-06\n",
      "PINN Training Completed\n",
      "\n",
      "PINN Training Time: 1.1804032559605548 Hours\n",
      "\n",
      "Current GPT-PINN Depth: [3,1,1]\n",
      "\n",
      "Begin GPT-PINN Training (Finding Neuron 2 / Largest Loss Training)\n",
      "arg: 0, residual loss: 4.34956655226415e-06, total loss: 9.260070328309666e-06\n",
      "arg: 1, residual loss: 5.016230352339335e-05, total loss: 5.686262738890946e-05\n",
      "arg: 3, residual loss: 5.473956844070926e-05, total loss: 6.22335501248017e-05\n",
      "arg: 6, residual loss: 4.877939136349596e-05, total loss: 6.34582684142515e-05\n",
      "arg: 7, residual loss: 5.7861514505930245e-05, total loss: 6.678431964246556e-05\n",
      "arg: 14, residual loss: 7.482113869627938e-05, total loss: 8.56089754961431e-05\n",
      "arg: 55, residual loss: 8.842210081638768e-05, total loss: 0.00010028311953647062\n",
      "arg: 389, residual loss: 8.909052121452987e-05, total loss: 0.00010623178241075948\n",
      "GPT-PINN Training Completed\n",
      "\n",
      "GPT Training Time (1 Neurons): 0.0021335539005586826 Hours\n",
      "\n",
      "Largest Loss (Using 1 Neurons): 0.00010623178241075948\n",
      "Parameter Case: 389\n",
      "******************************************************************\n",
      "Begin Full PINN Training: case=389 (Obtaining Neuron 2)\n",
      "Epoch: 0 | Loss: 2.5895848274230957\n",
      "Epoch: 10000 | loss: 5.765693458670285e-06\n",
      "Epoch: 20000 | loss: 1.2073348898411496e-06\n",
      "Epoch: 30000 | loss: 0.022122280672192574\n",
      "Epoch: 40000 | loss: 1.1333800102875102e-06\n",
      "Epoch: 50000 | loss: 5.562131377701007e-07\n",
      "Epoch: 60000 | loss: 1.4836033187748399e-05\n",
      "PINN Training Completed\n",
      "\n",
      "PINN Training Time: 1.1542208345372151 Hours\n",
      "\n",
      "Current GPT-PINN Depth: [3,2,1]\n",
      "\n",
      "Begin GPT-PINN Training (Finding Neuron 3 / Largest Loss Training)\n",
      "arg: 0, residual loss: 4.548928700387478e-06, total loss: 9.052781933860388e-06\n",
      "arg: 1, residual loss: 5.0388844101689756e-05, total loss: 5.68441282666754e-05\n",
      "arg: 7, residual loss: 5.555566167458892e-05, total loss: 6.0000991652486846e-05\n",
      "arg: 14, residual loss: 6.498671427834779e-05, total loss: 6.889191718073562e-05\n",
      "arg: 25, residual loss: 6.477478746091947e-05, total loss: 7.311254012165591e-05\n",
      "arg: 32, residual loss: 6.907094939379022e-05, total loss: 7.694680971326306e-05\n",
      "arg: 55, residual loss: 8.9789587946143e-05, total loss: 9.831071656662971e-05\n",
      "GPT-PINN Training Completed\n",
      "\n",
      "GPT Training Time (2 Neurons): 0.0020743333566739844 Hours\n",
      "\n",
      "Largest Loss (Using 2 Neurons): 9.831071656662971e-05\n",
      "Parameter Case: 55\n",
      "******************************************************************\n",
      "Begin Full PINN Training: case=55 (Obtaining Neuron 3)\n",
      "Epoch: 0 | Loss: 0.11311905086040497\n",
      "Epoch: 10000 | loss: 2.3856227926444262e-06\n",
      "Epoch: 20000 | loss: 2.1099156128912e-06\n",
      "Epoch: 30000 | loss: 2.537485670472961e-06\n",
      "Epoch: 40000 | loss: 5.028182386013214e-07\n",
      "Epoch: 50000 | loss: 5.4266115512291435e-06\n",
      "Epoch: 60000 | loss: 3.283555997768417e-06\n",
      "PINN Training Completed\n",
      "\n",
      "PINN Training Time: 1.5414482418402784 Hours\n",
      "\n",
      "Current GPT-PINN Depth: [3,3,1]\n",
      "\n",
      "Begin GPT-PINN Training (Finding Neuron 4 / Largest Loss Training)\n",
      "arg: 0, residual loss: 4.61206855106866e-06, total loss: 8.96257370186504e-06\n",
      "arg: 1, residual loss: 4.330820229370147e-05, total loss: 4.689922570833005e-05\n",
      "arg: 5, residual loss: 4.268059637979604e-05, total loss: 4.982923201168887e-05\n",
      "arg: 7, residual loss: 5.4161148000275716e-05, total loss: 5.941883864579722e-05\n",
      "arg: 16, residual loss: 5.899856478208676e-05, total loss: 6.212145672179759e-05\n",
      "arg: 25, residual loss: 6.523307092720643e-05, total loss: 7.225290028145537e-05\n",
      "arg: 32, residual loss: 6.949418457224965e-05, total loss: 7.675137021578848e-05\n",
      "arg: 418, residual loss: 7.684216689085588e-05, total loss: 8.446201536571607e-05\n",
      "GPT-PINN Training Completed\n",
      "\n",
      "GPT Training Time (3 Neurons): 0.0024249203705565174 Hours\n",
      "\n",
      "Largest Loss (Using 3 Neurons): 8.446201536571607e-05\n",
      "Parameter Case: 418\n",
      "******************************************************************\n",
      "Begin Final Full PINN Training: case=418 (Obtaining Neuron 4)\n",
      "Epoch: 0 | Loss: 1.1949989795684814\n",
      "Epoch: 10000 | loss: 5.7607035159890074e-06\n",
      "Epoch: 20000 | loss: 0.0001210479240398854\n",
      "Epoch: 30000 | loss: 8.059539686655626e-05\n",
      "Epoch: 40000 | loss: 1.2596500710060354e-05\n",
      "Epoch: 50000 | loss: 2.680025772860972e-06\n",
      "Epoch: 60000 | loss: 2.920233328040922e-06\n",
      "PINN Training Completed\n",
      "\n",
      "PINN Training Time: 1.3494234395024898 Hours\n",
      "\n",
      "Current GPT-PINN Depth: [3,4,1]\n",
      "\n",
      "Begin Final GPT-PINN Training (Largest Loss Training)\n",
      "arg: 0, residual loss: 4.645210538001265e-06, total loss: 8.81177038536407e-06\n",
      "arg: 1, residual loss: 4.22596131102182e-05, total loss: 4.501875810092315e-05\n",
      "arg: 5, residual loss: 4.065833491040394e-05, total loss: 4.5125969336368144e-05\n",
      "arg: 7, residual loss: 5.262042031972669e-05, total loss: 5.597632480203174e-05\n",
      "arg: 16, residual loss: 5.7492412452120334e-05, total loss: 5.9418460296001285e-05\n",
      "arg: 32, residual loss: 6.981171463849023e-05, total loss: 7.666856981813908e-05\n",
      "GPT-PINN Training Completed\n",
      "\n",
      "GPT Training Time (4 Neurons): 0.0016400897108380579 Hours\n",
      "\n",
      "Largest Loss (Using 4 Neurons): 7.666856981813908e-05\n",
      "Parameter Case: 32\n",
      "******************************************************************\n",
      "*** Full PINN and GPT-PINN Training Complete ***\n",
      "Total Training Time: 5.233844927534713 Hours\n",
      "\n",
      "Final GPT-PINN Depth: [2, 4, 1]\n"
     ]
    }
   ],
   "source": [
    "pinn_train_times = np.ones(number_of_neurons)\n",
    "gpt_train_times  = np.ones(number_of_neurons)\n",
    "arg_max = 0\n",
    "num_pre_neurons = 0\n",
    "total_train_time_1 = time.perf_counter()\n",
    "###############################################################################\n",
    "################################ Training Loop ################################\n",
    "###############################################################################\n",
    "for i in range(number_of_neurons):\n",
    "    print(\"******************************************************************\")\n",
    "    ########################### Full PINN Training ############################\n",
    "    \n",
    "    if i < num_pre_neurons:\n",
    "        path = fr\"../data/Full-PINN-Data (Burgers) (K={K})/({i+1})\"\n",
    "        weights = []\n",
    "        bias = []\n",
    "        for k in range(len(layers_pinn)-1):\n",
    "            weights.append(np.loadtxt(fr\"{path}/saved_w{k+1}.txt\"))\n",
    "            bias.append(np.loadtxt(fr\"{path}/saved_b{k+1}.txt\"))\n",
    "        P_list[i] = P(layers_pinn, weights, bias).to(device)\n",
    "        P_resid_vorstr_values[:, :, i] = P_list[i](x_resid, y_resid, t_resid).detach()\n",
    "        \n",
    "        layers_gpt = np.array([2, i+1, 1])\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        curl_f_pinn = curl_f_train[:, :, arg_max].reshape(-1, 1)\n",
    "        \n",
    "        # weights_pde = 100*torch.rand(Nc, 1).to(device)\n",
    "        # weights_IC  = torch.rand(Nc, 1).to(device)\n",
    "        \n",
    "        pinn = PINN(layers_pinn, nu, curl_f_pinn, omega0).to(device)\n",
    "        pinn_train_time_1 = time.perf_counter()\n",
    "        \n",
    "        if (i+1 == number_of_neurons):\n",
    "            print(f\"Begin Final Full PINN Training: case={arg_max} (Obtaining Neuron {i+1})\")\n",
    "        else:\n",
    "            print(f\"Begin Full PINN Training: case={arg_max} (Obtaining Neuron {i+1})\")\n",
    "        \n",
    "        pinn_losses = pinn_train(pinn, nu, xt_resid, IC_xt, BC_xt_bottom, BC_xt_top,\n",
    "                                 BC_xt_left, BC_xt_right, epochs_pinn, lr_adam, tol_adam)\n",
    "\n",
    "        pinn_train_time_2 = time.perf_counter()\n",
    "        print(f\"PINN Training Time: {(pinn_train_time_2-pinn_train_time_1)/3600} Hours\")\n",
    "        \n",
    "        weights = []\n",
    "        bias    = []\n",
    "        \n",
    "        for k in range(len(layers_pinn)-1):\n",
    "            weights.append(pinn.linears[k].weight.detach().cpu())\n",
    "            bias.append(pinn.linears[k].bias.detach().cpu())\n",
    "        \n",
    "        P_list[i] = P(layers_pinn, weights, bias).to(device)\n",
    "\n",
    "        print(f\"\\nCurrent GPT-PINN Depth: [3,{i+1},1]\")\n",
    "        \n",
    "        if (save_data):        \n",
    "            path = fr\"../data/Full-PINN-Data (Burgers) (K={K})/({i+1})\"\n",
    "            \n",
    "            if not os.path.exists(path):\n",
    "                os.makedirs(path)\n",
    "            for k in range(len(layers_pinn)-1):\n",
    "                np.savetxt(fr\"{path}/saved_w{k+1}.txt\", weights[k].numpy())\n",
    "                np.savetxt(fr\"{path}/saved_b{k+1}.txt\", bias[k].numpy())\n",
    "      \n",
    "        if (plot_pinn_sol):\n",
    "            x_test = xt_test[:,0].view(-1).cpu().detach().numpy()\n",
    "            y_test = xt_test[:,1].view(-1).cpu().detach().numpy()\n",
    "            t_test = xt_test[:,2].view(-1).cpu().detach().numpy()\n",
    "            X_star = np.hstack((x_test[:,None], y_test[:, None], t_test[:,None]))\n",
    "            u = pinn(X_star)\n",
    "        \n",
    "            Burgers_plot(t_test, x_test, u, title=fr\"SA-PINN Solution case: {arg_max}\")\n",
    "    \n",
    "        \n",
    "    \n",
    "        if (i == number_of_neurons-1) and (train_final_gpt == False):\n",
    "            break\n",
    "\n",
    "        ############################ GPT-PINN Training ############################\n",
    "        layers_gpt = np.array([3, i+1, 1])\n",
    "        \n",
    "        P_t, P_x, P_xx, P_y, P_yy, vel, lap_psi = autograd_calculations(x_resid, y_resid, t_resid, P_list[i]) \n",
    "        Pt_nu_lap_omega[:, i][:, None] = Pt_nu_lap_vor(nu, P_t, P_xx, P_yy)\n",
    "   \n",
    "        P_t_term[:,i][:,None]  = P_t # torch.from_numpy(u_t)\n",
    "        P_x_term[:,i][:,None]  = P_x # torch.from_numpy(u_x)\n",
    "        P_xx_term[:,i][:,None] = P_xx # torch.from_numpy(u_xx)\n",
    "        P_y_term[:,i][:,None]  = P_y \n",
    "        P_yy_term[:,i][:,None] = P_yy\n",
    "        \n",
    "        \n",
    "        P_IC_values[:, i][:,None] = P_list[i](IC_x, IC_y, IC_t)[:, 0:1].detach() \n",
    "        P_BC_top[:, :, i]    = P_list[i](BC_x_top, BC_y_top, BC_t_top).detach() \n",
    "        P_BC_bottom[:, :, i] = P_list[i](BC_x_bottom, BC_y_bottom, BC_t_bottom).detach()\n",
    "        P_BC_left[:, :, i]   = P_list[i](BC_x_left, BC_y_left, BC_t_left).detach()\n",
    "        P_BC_right[:, :, i]  = P_list[i](BC_x_right, BC_y_right, BC_t_right).detach()\n",
    "        \n",
    "        P_resid_vorstr_values[:, :, i] = P_list[i](x_resid, y_resid, t_resid).detach()\n",
    "        P_vel[:, :, i] = vel\n",
    "        P_lap_psi[:, i][:, None] = lap_psi\n",
    "        \n",
    "        # Finding The Next Neuron\n",
    "        largest_case = 0\n",
    "        largest_loss = 0\n",
    "\n",
    "        if (i+1 == number_of_neurons):\n",
    "            print(\"\\nBegin Final GPT-PINN Training (Largest Loss Training)\")\n",
    "        else:\n",
    "            print(f\"\\nBegin GPT-PINN Training (Finding Neuron {i+2} / Largest Loss Training)\")\n",
    "\n",
    "        gpt_train_time_1 = time.perf_counter()\n",
    "        for j in range(n_train):\n",
    "\n",
    "            c_initial = torch.full((1, i+1), 1/(i+1))\n",
    "            curl_f_gpt = curl_f_train[:, :, j].reshape(-1, 1)\n",
    "            \n",
    "            P_resid_gpt = P_resid_vorstr_values[:, :, :i+1]\n",
    "            P_IC_gpt    = P_IC_values[:, :i+1]\n",
    "            P_BC_bottom_gpt = P_BC_bottom[:, :, :i+1]\n",
    "            P_BC_top_gpt    = P_BC_top[:, :, :i+1]\n",
    "            P_BC_left_gpt  = P_BC_left[:, :, :i+1]\n",
    "            P_BC_right_gpt = P_BC_right[:, :, :i+1]\n",
    "            P_vel_gpt = P_vel[:, :, :i+1]\n",
    "            P_x_gpt   = P_x_term[:, :i+1]\n",
    "            P_y_gpt   = P_y_term[:, :i+1]\n",
    "            P_lap_psi_gpt = P_lap_psi[:, :i+1]\n",
    "            Pt_nu_lap_omega_gpt = Pt_nu_lap_omega[:, :i+1]\n",
    "\n",
    "            # GPT_NN = TGPT(layers_gpt, nu, P_list[0:i+1], c_initial, u0_gpt_train, f_hat).to(device)\n",
    "\n",
    "            GPT_NN = GPT(layers_gpt, nu, P_list[0:i+1], c_initial, omega0, curl_f_gpt, P_resid_gpt, \n",
    "                         P_IC_gpt, P_BC_bottom_gpt, P_BC_top_gpt, P_BC_left_gpt, P_BC_right_gpt, Pt_nu_lap_omega_gpt, P_vel_gpt, P_x_gpt, P_y_gpt, P_lap_psi_gpt).to(device)\n",
    "            gpt_losses = gpt_train(GPT_NN, nu, xt_resid, IC_xt, BC_xt_bottom, BC_xt_top, BC_xt_left,        \n",
    "                                   BC_xt_right, epochs_gpt, lr_gpt, j, largest_loss, largest_case)\n",
    "            \n",
    "            largest_loss = gpt_losses[0]\n",
    "            largest_case = gpt_losses[1]\n",
    "            arg_max = largest_case\n",
    "        gpt_train_time_2 = time.perf_counter()\n",
    "        print(\"GPT-PINN Training Completed\")\n",
    "        print(f\"\\nGPT Training Time ({i+1} Neurons): {(gpt_train_time_2-gpt_train_time_1)/3600} Hours\")\n",
    "        \n",
    "        loss_list[i] = largest_loss\n",
    "        \n",
    "            \n",
    "        print(f\"\\nLargest Loss (Using {i+1} Neurons): {largest_loss}\")\n",
    "        print(f\"Parameter Case: {largest_case}\")\n",
    "total_train_time_2 = time.perf_counter()                       \n",
    "\n",
    "###############################################################################\n",
    "# Results of largest loss, parameters chosen, and times may vary based on\n",
    "# the initialization of full PINN and the final loss of the full PINN\n",
    "print(\"******************************************************************\")\n",
    "print(\"*** Full PINN and GPT-PINN Training Complete ***\")\n",
    "print(f\"Total Training Time: {(total_train_time_2-total_train_time_1)/3600} Hours\\n\")\n",
    "print(f\"Final GPT-PINN Depth: {[2,len(P_list),1]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
